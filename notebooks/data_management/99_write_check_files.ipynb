{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97f087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n"
     ]
    }
   ],
   "source": [
    "%run \"match_sample_files.ipynb\"\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import collections as col\n",
    "\n",
    "DRY_RUN = False\n",
    "\n",
    "MANUALLY_CURATED_SOURCES = {\n",
    "    \"UW_WH\": (\"UW\", \"hgsvc\"),\n",
    "}\n",
    "\n",
    "fofn_files = []\n",
    "fofn_files_out = PROJECT_BASE.joinpath(\"samples\", \"fofn_table.tsv\")\n",
    "\n",
    "for sample_name, sample_obj in SAMPLES.items():\n",
    "    if sample_name == sample_obj.alt:\n",
    "        continue\n",
    "    \n",
    "    sample_files = []\n",
    "    sample_files_main = FILES_EXIST_PER_SAMPLE[sample_name]\n",
    "    sample_files_alt = FILES_EXIST_PER_SAMPLE[sample_obj.alt]\n",
    "    \n",
    "    for sf_main in sample_files_main:\n",
    "        if sf_main in sample_files:\n",
    "            continue\n",
    "        sample_files.append(sf_main)\n",
    "    \n",
    "    for sf_alt in sample_files_alt:\n",
    "        if sf_alt in sample_files:\n",
    "            continue\n",
    "        sample_files.append(sf_alt)\n",
    "                \n",
    "    assert len(sample_files) == len(sample_files_main) + len(sample_files_alt)\n",
    "    \n",
    "    for fn, sample_file in enumerate(sample_files, start=0):\n",
    "        key = sample_file.read_type, sample_file.file_group\n",
    "        sample_obj.sample_files[key].append(sample_file.data_rel_path)\n",
    "        if fn == 0:\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.group_date\n",
    "            )\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.file_date\n",
    "            )\n",
    "        else:\n",
    "            sample_obj.source_dates[sample_file.file_group].append(\n",
    "                sample_file.file_date\n",
    "            )\n",
    "            \n",
    "    \n",
    "    if sample_obj.hifi_complete:\n",
    "        for (read_type, read_group), read_files in sample_obj.sample_files.items():\n",
    "            if read_type != \"hifi\":\n",
    "                continue\n",
    "            source_dates = sample_obj.source_dates[read_group]\n",
    "            try:\n",
    "                source_date = str(int(source_dates[0]))\n",
    "            except ValueError:\n",
    "                source_date, _ = col.Counter(source_dates).most_common(1)[0]\n",
    "                \n",
    "            read_group_info = MERGED.loc[MERGED[\"file_group\"] == read_group, [\"data_source\", \"project\"]]\n",
    "            if read_group_info.empty:\n",
    "                read_group_info = set(\n",
    "                    ALL_FILES.loc[ALL_FILES[\"file_group\"] == read_group, \"curated_by\"].values\n",
    "                )\n",
    "                data_source, project = MANUALLY_CURATED_SOURCES[read_group_info]\n",
    "            else:\n",
    "                data_source = set(read_group_info[\"data_source\"].values).pop()\n",
    "                project = set(read_group_info[\"project\"].values).pop()\n",
    "                \n",
    "            if data_source == project:\n",
    "                assert data_source.startswith(\"HGSVC\") and project.startswith(\"HGSVC\")\n",
    "                project = \"hgsvc\"\n",
    "                data_source = data_source[5:].upper()\n",
    "                \n",
    "            source_path = sample_obj.get_file_group_lca_path(read_files)\n",
    "            verified_file_path = PATH_PREFIX.local.joinpath(\n",
    "                DATA_ROOT, source_path\n",
    "            )\n",
    "            \n",
    "            verified_file_name = f\"{sample_obj.name}.{len(read_files)}-cells.verified\"\n",
    "            verified_read_files = sorted([fp.name for fp in read_files])\n",
    "            if not DRY_RUN:\n",
    "                assert verified_file_path.is_dir()\n",
    "                with open(verified_file_path.joinpath(verified_file_name), \"w\") as dump:\n",
    "                    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "                    _ = dump.write(\"\\n\".join(verified_read_files))\n",
    "                    \n",
    "            fofn_name = f\"{sample_obj.name}_{read_type}_fastq.{project}-{data_source}-{source_date}.fofn\"\n",
    "\n",
    "            fofn_path = LOCAL_SAMPLE_ROOT.joinpath(\n",
    "                sample_obj.name, fofn_name\n",
    "            )\n",
    "            \n",
    "            if not DRY_RUN:\n",
    "                with open(fofn_path, \"w\") as dump:\n",
    "                    for rf in sorted(read_files):\n",
    "                        assert str(rf).startswith(\"project-centric\")\n",
    "                        _ = dump.write(str(rf) + \"\\n\")\n",
    "            \n",
    "            if sample_obj.ont_complete:\n",
    "                remote_fofn_path = str(fofn_path).replace(str(PATH_PREFIX.local), str(PATH_PREFIX.remote))\n",
    "                fofn_files.append((sample_obj.name, read_type, remote_fofn_path))\n",
    "                \n",
    "    if sample_obj.ont_complete:\n",
    "        for (read_type, read_group), read_files in sample_obj.sample_files.items():\n",
    "            if read_type != \"ont\":\n",
    "                continue\n",
    "            source_dates = sample_obj.source_dates[read_group]\n",
    "            try:\n",
    "                source_date = str(int(source_dates[0]))\n",
    "            except ValueError:\n",
    "                source_date, _ = col.Counter(source_dates).most_common(1)[0]\n",
    "            \n",
    "            read_group_info = MERGED.loc[MERGED[\"file_group\"] == read_group, [\"data_source\", \"project\"]]\n",
    "            if read_group_info.empty:\n",
    "                read_group_info = set(\n",
    "                    ALL_FILES.loc[ALL_FILES[\"file_group\"] == read_group, \"curated_by\"].values\n",
    "                )\n",
    "            else:\n",
    "                data_source = set(read_group_info[\"data_source\"].values).pop()\n",
    "                project = set(read_group_info[\"project\"].values).pop()\n",
    "\n",
    "            if data_source == project:\n",
    "                assert data_source.startswith(\"HGSVC\") and project.startswith(\"HGSVC\")\n",
    "                project = \"hgsvc\"\n",
    "                data_source = data_source[5:].upper()\n",
    "               \n",
    "            source_path = sample_obj.get_file_group_lca_path(read_files)\n",
    "            verified_file_path = PATH_PREFIX.local.joinpath(\n",
    "                DATA_ROOT, source_path\n",
    "            )\n",
    "            \n",
    "            verified_file_name = f\"{sample_obj.name}.{len(read_files)}-cells.verified\"\n",
    "            verified_read_files = sorted([fp.name for fp in read_files])\n",
    "            \n",
    "            if not DRY_RUN:\n",
    "                assert verified_file_path.is_dir()\n",
    "                with open(verified_file_path.joinpath(verified_file_name), \"w\") as dump:\n",
    "                    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "                    _ = dump.write(\"\\n\".join(verified_read_files))\n",
    "\n",
    "            fofn_name = f\"{sample_obj.name}_{read_type}_fastq.{project}-{data_source}-{source_date}.fofn\"\n",
    "\n",
    "            fofn_path = LOCAL_SAMPLE_ROOT.joinpath(\n",
    "                sample_obj.name, fofn_name\n",
    "            )\n",
    "\n",
    "            if not DRY_RUN:\n",
    "                with open(fofn_path, \"w\") as dump:\n",
    "                    for rf in sorted(read_files):\n",
    "                        assert str(rf).startswith(\"project-centric\")\n",
    "                        _ = dump.write(str(rf) + \"\\n\")\n",
    "            \n",
    "            if sample_obj.hifi_complete:\n",
    "            \n",
    "                remote_fofn_path = str(fofn_path).replace(str(PATH_PREFIX.local), str(PATH_PREFIX.remote))\n",
    "                fofn_files.append((sample_obj.name, read_type, remote_fofn_path))\n",
    "            \n",
    "fofn_files = pd.DataFrame.from_records(fofn_files, columns=[\"sample\", \"read_type\", \"fofn_path\"])\n",
    "with open(fofn_files_out, \"w\") as dump:\n",
    "    _ = dump.write(f\"# {TODAY}\\n\")\n",
    "    fofn_files.to_csv(dump, sep=\"\\t\", header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
