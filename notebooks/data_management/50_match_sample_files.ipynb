{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e2a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping over 19320 - not enough data\n",
      "\n",
      "Skipping over 19320 - not enough data\n"
     ]
    }
   ],
   "source": [
    "%run \"../00_project_config.ipynb\"\n",
    "%run \"05_annotations.ipynb\"\n",
    "%run \"10_data_objects.ipynb\"\n",
    "%run \"20_process_sample_table.ipynb\"\n",
    "%run \"30_process_data_sources.ipynb\"\n",
    "\n",
    "MANUALLY_CURATED_FOLDERS = {\n",
    "    \"20230501_HGSVC_UL_ONT-UW\": \"UW_WH\",\n",
    "    \"20211013_ONT_Rebasecalled\": \"JAX_PA\",\n",
    "    \"20230706_HGSVC_EEE_UL_ONT\": \"UW_WH\",\n",
    "    \"20230703_HGSVC_EEE_HIFI\": \"UW_WH\",\n",
    "    \"20220831_JAX_HiFi\": \"JAX_PA\"\n",
    "}\n",
    "\n",
    "AUTO_TABLE_ALL_KNOWN = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"annotated_files.tsv\")\n",
    "AUTO_TABLE_ALL_KNOWN.parent.mkdir(parents=True, exist_ok=True)\n",
    "AUTO_TABLE_ALL_FILES = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"existing_files.tsv\")\n",
    "AUTO_TABLE_ALL_FILES.parent.mkdir(parents=True, exist_ok=True)\n",
    "AUTO_TABLE_ERR_FILES = PROJECT_BASE.joinpath(\"annotations\", \"autogen\", \"error_files.tsv\")\n",
    "AUTO_TABLE_ERR_FILES.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matched_records = []\n",
    "matched_files = []\n",
    "for row in KNOWN_FILES.itertuples(index=True):\n",
    "    \n",
    "    files_for_sample = FILES_EXIST_PER_SAMPLE[row.sample]\n",
    "    matched_files = []\n",
    "    num_matches = 0\n",
    "    sample_batch = SAMPLES[row.sample].batch_num\n",
    "    for file in files_for_sample:\n",
    "        if row.read_type != file.read_type:\n",
    "            continue\n",
    "        if row.cell in file.file_name:\n",
    "            assert row.read_type == file.read_type\n",
    "            matched_records.append(\n",
    "                (row.Index, sample_batch, file.data_rel_path, file.file_date, file.file_group, file.group_date, 1)\n",
    "            )\n",
    "            matched_files.append(file)\n",
    "            num_matches += 1\n",
    "    if num_matches > 1:\n",
    "        for mf in matched_files:\n",
    "            print(mf)\n",
    "        raise ValueError(f\"Multi-match: {row}\")\n",
    "        \n",
    "    if num_matches == 1:\n",
    "        matched_files[0].set_matched_entry(row.Index)\n",
    "        matched_files[0].set_curator(\"metadata_table\")\n",
    "    \n",
    "matched_records = pd.DataFrame.from_records(\n",
    "    matched_records,\n",
    "    columns=[\"index\", \"processing_batch\", \"file_rel_path\", \"file_date\", \"file_group\", \"group_date\", \"matched\"]\n",
    ")\n",
    "matched_records.index = matched_records[\"index\"]\n",
    "matched_records.drop(\"index\", inplace=True, axis=1)\n",
    "\n",
    "MERGED = KNOWN_FILES.merge(matched_records, how=\"outer\", left_index=True, right_index=True)\n",
    "MERGED[\"matched\"].fillna(0, inplace=True)\n",
    "MERGED[\"processing_batch\"].fillna(-1, inplace=True)\n",
    "MERGED.fillna(\"n/a\", inplace=True)\n",
    "MERGED[\"matched\"] = MERGED[\"matched\"].astype(int)\n",
    "MERGED[\"processing_batch\"] = MERGED[\"processing_batch\"].astype(int)\n",
    "\n",
    "all_files_table = []\n",
    "for sample, files_for_sample in FILES_EXIST_PER_SAMPLE.items():\n",
    "    for sample_file in files_for_sample:\n",
    "        if sample_file.matched_entry is None:\n",
    "            for folder, curator in MANUALLY_CURATED_FOLDERS.items():\n",
    "                if folder in str(sample_file.data_rel_path):\n",
    "                    sample_file.set_matched_entry(-1)\n",
    "                    sample_file.set_curator(curator)\n",
    "                    break\n",
    "            if sample_file.matched_entry is None:\n",
    "                sample_file.set_curator(\"error\")\n",
    "        all_files_table.append(sample_file.get_table_row())\n",
    "all_files_header = sample_file.get_table_header()\n",
    "\n",
    "with open(AUTO_TABLE_ALL_KNOWN, \"w\") as dump_table:\n",
    "    _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "    MERGED.to_csv(\n",
    "        dump_table,\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=True,\n",
    "        index_label=\"index\"\n",
    "    )\n",
    "    \n",
    "ALL_FILES = pd.DataFrame.from_records(\n",
    "    all_files_table,\n",
    "    columns=all_files_header\n",
    ")\n",
    "ALL_FILES.sort_values([\"sample\", \"read_type\", \"file_group\", \"file_name\"], inplace=True)\n",
    "with open(AUTO_TABLE_ALL_FILES, \"w\") as dump_table:\n",
    "    _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "    ALL_FILES.to_csv(\n",
    "        dump_table,\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "ERR_FILES = ALL_FILES.loc[ALL_FILES[\"curated_by\"] == \"error\", :].copy()\n",
    "with open(AUTO_TABLE_ERR_FILES, \"w\") as dump_table:\n",
    "    _ = dump_table.write(f\"# {TODAY}\\n\")\n",
    "    ERR_FILES.to_csv(\n",
    "        dump_table,\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
