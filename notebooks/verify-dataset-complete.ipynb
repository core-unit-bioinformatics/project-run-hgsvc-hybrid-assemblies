{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd583875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230425T1953\n",
      "no year  project-centric/unknown/nanopore/NA24385_HG002\n",
      "no year  project-centric/unknown/pacbio_hifi/HG00733\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA586863/nanopore\n",
      "no year  project-centric/PRJNA586863/pacbio_hifi\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA731524/nanopore\n",
      "no year  project-centric/PRJNA731524/pacbio_hifi\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA813010/nanopore\n",
      "no year  project-centric/PRJNA813010/pacbio_hifi\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "import itertools as itt\n",
    "\n",
    "DRYRUN = False\n",
    "VERBOSE = False\n",
    "\n",
    "ts = dt.datetime.now()\n",
    "TIMESTAMP = ts.strftime(\"%Y%m%dT%H%M\")\n",
    "print(TIMESTAMP)\n",
    "\n",
    "MOUNT = pl.Path(\"/mounts/hilbert/project\")\n",
    "TOP_LEVEL = pl.Path(\"projects/medbioinf/data/00_RESTRUCTURE\")\n",
    "project_folders = [\n",
    "    \"hgsvc\", \"unknown\", \"PRJNA586863\", \"PRJNA731524\", \"PRJNA813010\"\n",
    "]\n",
    "data_folders = [\"nanopore\", \"pacbio_hifi\"]\n",
    "\n",
    "cell_metadata = pl.Path(\"/home/ebertp/work/code/cubi/project-run-hgsvc-hybrid-assemblies/annotations/external\")\n",
    "\n",
    "uw_complete_ont_samples_202304 = cell_metadata.joinpath(\"20230425_uw_ont-complete-samples.txt\")\n",
    "\n",
    "curated_folders = [\n",
    "    (\n",
    "        \"project-centric/hgsvc/nanopore/20230414_HGSVC_UL_ONT\",\n",
    "        \"UW\",\n",
    "        \"ONT\",\n",
    "        \"curated_WH\",\n",
    "        uw_complete_ont_samples_202304\n",
    "    ),\n",
    "]\n",
    "curated_folder_names = [cf[0] for cf in curated_folders]\n",
    "\n",
    "sample_order_file = cell_metadata.parent.joinpath(\"hgsvc_sample_order.tsv\")\n",
    "sample_order = pd.read_csv(sample_order_file, sep=\"\\t\", header=0)\n",
    "sample_order[\"sin\"] = \"SIN:\" + sample_order[\"sample\"].str.extract(\"([0-9]+)\")\n",
    "sample_order.drop(\"sample\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "hifi_cells = cell_metadata.glob(\"*hifi*.tsv\")\n",
    "ont_cells = cell_metadata.glob(\"*ont*.tsv\")\n",
    "\n",
    "relabel_file = cell_metadata.joinpath(\"relabel_sources.tsv\")\n",
    "\n",
    "clean_out = cell_metadata.parent.joinpath(\"hgsvc_cells.tsv\")\n",
    "error_out = cell_metadata.parent.joinpath(\"errors.tsv\")\n",
    "\n",
    "PROBLEM_SAMPLES = []\n",
    "\n",
    "def load_cell_metadata(fpath):\n",
    "    \n",
    "    source_must_exist = False\n",
    "    if \"jax\" in fpath.name:\n",
    "        source = \"JAX\"\n",
    "    elif \"uwash\" in fpath.name or \"UW\" in fpath.name:\n",
    "        source = \"UW\"\n",
    "    elif \"umigs\" in fpath.name:\n",
    "        source = \"UMIGS\"\n",
    "    else:\n",
    "        source_must_exist = True\n",
    "        #raise ValueError(f\"Unknown project {fpath.name}\")\n",
    "    if \"hifi\" in fpath.name:\n",
    "        read_type = \"HiFi\"\n",
    "    elif \"ont\" in fpath.name:\n",
    "        read_type = \"ONT\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown read type: {fpath.name}\")\n",
    "    \n",
    "    df = pd.read_csv(fpath, header=0, sep=\"\\t\", comment=\"#\")\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if source_must_exist:\n",
    "        assert \"source\" in df.columns\n",
    "        df = df[[\"sample\", \"cell\", \"source\"]]\n",
    "    else:\n",
    "        df = df[[\"sample\", \"cell\"]]\n",
    "        df[\"source\"] = source\n",
    "    df[\"sample\"] = df[\"sample\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.extract(\"([A-Z\\-_a-z0-9]+)\")\n",
    "    df[\"sin\"] = \"SIN:\" + df[\"sample\"].str.extract(\"([0-9]+)\") \n",
    "    df[\"read_type\"] = read_type\n",
    "    df[\"annotation_source\"] = fpath.name\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_fastq_files(folder_path):\n",
    "    \n",
    "    all_files = list(folder_path.glob(\"**/*.fastq.gz\"))\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"no fastqs at {folder_path}\")\n",
    "    pass_files = [fp for fp in all_files if \"fail\" not in fp.name]\n",
    "    all_names = [fp.name for fp in pass_files]\n",
    "    return pass_files\n",
    "\n",
    "\n",
    "def load_curated_samples(file_path, sample_source, read_type, annotation_source):\n",
    "    \n",
    "    samples = []\n",
    "    with open(file_path, \"r\") as listing:\n",
    "        for line in listing:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            sample = line.strip()\n",
    "            sin = sample[2:]\n",
    "            assert int(sin)\n",
    "            samples.append(\n",
    "                (\n",
    "                    sample, f\"SIN:{sin}\",\n",
    "                    sample_source, read_type,\n",
    "                    annotation_source\n",
    "                )\n",
    "            )\n",
    "    df = pd.DataFrame.from_records(\n",
    "        samples, columns=[\"sample\", \"sin\", \"source\", \"read_type\", \"annotation_source\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_curated_data_files(curated_folder):\n",
    "    \n",
    "    subfolder, source, read_type, annotation_source, curated_samples = curated_folder\n",
    "    data_files = load_fastq_files(MOUNT.joinpath(TOP_LEVEL, subfolder))\n",
    "    samples = load_curated_samples(curated_samples, source, read_type, annotation_source)\n",
    "    \n",
    "    local_files = []\n",
    "    for data_file in data_files:\n",
    "        sample = data_file.parent.name\n",
    "        sin = sample[2:]\n",
    "        assert int(sin)\n",
    "        sin = f\"SIN:{sin}\"\n",
    "        cell = data_file.name\n",
    "        assert cell.endswith(\".fastq.gz\")\n",
    "        cell = cell.rsplit(\".\", 2)[0]\n",
    "        local_files.append(\n",
    "            (\n",
    "                sample, sin, cell, source, read_type, annotation_source\n",
    "            )\n",
    "        )\n",
    "    local_files = pd.DataFrame.from_records(\n",
    "        local_files, columns=[\"sample\", \"sin\", \"cell\", \"source\", \"read_type\", \"annotation_source\"]\n",
    "    )\n",
    "    common_columns = [c for c in local_files.columns if c in samples.columns]\n",
    "    samples = samples.merge(local_files, on=common_columns, how=\"outer\")\n",
    "    assert not pd.isnull(samples).any(axis=1).any()\n",
    "    return samples\n",
    "\n",
    "\n",
    "def replace_curated_cells(cell_table, curated_files, read_type):\n",
    "    \n",
    "    to_drop = []\n",
    "    for (sin, source, read_type), cells in cell_table.groupby([\"sin\", \"source\", \"read_type\"]):\n",
    "        if read_type != read_type:\n",
    "            continue\n",
    "        select_sin = curated_files[\"sin\"] == sin\n",
    "        select_source = curated_files[\"source\"] == source\n",
    "        select_reads = curated_files[\"read_type\"] == read_type\n",
    "        selector = select_sin & select_source & select_reads\n",
    "        if not selector.any():\n",
    "            continue\n",
    "            \n",
    "        to_drop.extend(cells.index.tolist())\n",
    "    \n",
    "    to_append = curated_files.loc[curated_files[\"read_type\"] == read_type, :].copy()\n",
    "    cell_table = cell_table.loc[~cell_table.index.isin(to_drop), :].copy()\n",
    "    cell_table = pd.concat([cell_table, to_append], ignore_index=False)\n",
    "    return cell_table\n",
    "\n",
    "\n",
    "def relabel_sources(labeling, cell_table):\n",
    "    \n",
    "    new_labels = pd.read_csv(labeling, header=0, sep=\"\\t\")\n",
    "    new_labels[\"cell\"] = new_labels[\"cell\"].str.strip()\n",
    "    \n",
    "    for row in new_labels.itertuples(index=False):\n",
    "        select_sample = cell_table[\"sample\"] == row.sample\n",
    "        select_cell = cell_table[\"cell\"].str.contains(row.cell)\n",
    "        select_reads = cell_table[\"read_type\"] == row.read_type\n",
    "        selector = select_sample & select_cell & select_reads\n",
    "        cell_table.loc[selector, \"source\"] = row.source\n",
    "    return cell_table\n",
    "\n",
    "\n",
    "curated_cells = pd.concat(\n",
    "    [load_curated_data_files(cf) for cf in curated_folders],\n",
    "    ignore_index=False\n",
    ")\n",
    "\n",
    "hifi_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in hifi_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "hifi_cells = relabel_sources(relabel_file, hifi_cells)\n",
    "hifi_cells.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ont_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in ont_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "ont_cells = relabel_sources(relabel_file, ont_cells)\n",
    "ont_cells.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hifi_cells = replace_curated_cells(hifi_cells, curated_cells, \"HiFi\")\n",
    "ont_cells = replace_curated_cells(ont_cells, curated_cells, \"ONT\")\n",
    "\n",
    "hifi_cells[\"HHU_complete\"] = \"no\"\n",
    "ont_cells[\"HHU_complete\"] = \"no\"\n",
    "hifi_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "hifi_cells.reset_index(drop=True, inplace=True)\n",
    "ont_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "ont_cells.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def group_files_by_sample_and_source(fastq_files, all_known, curated_folder=False):\n",
    "    \n",
    "    sample_file_groups = col.defaultdict(list)\n",
    "    for fq in fastq_files:\n",
    "        matches = []\n",
    "        for row in all_known.itertuples():\n",
    "            if row.cell not in fq.name:\n",
    "                continue\n",
    "            matches.append((row.sample, row.cell, row.source, fq, row.annotation_source))\n",
    "        if len(matches) > 1:\n",
    "            for smp, cell, source, fp, annfile in matches:\n",
    "                print(smp, ' - ', cell, ' - ', source, ' - ', fp.name, ' - ', annfile)\n",
    "            raise ValueError(\"Multi-match\")\n",
    "        elif len(matches) == 0 and curated_folder:\n",
    "            # a curated folder is assumed to contain only\n",
    "            # complete samples, must match\n",
    "            raise ValueError(f\"No match for curated sample/file: {fq}\")\n",
    "        elif len(matches) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sample, cell_id, source, file_path, annfile = matches[0]\n",
    "            sample_file_groups[(sample, source)].append(file_path)\n",
    "    return sample_file_groups\n",
    "\n",
    "\n",
    "def find_matching(sample_files, known_subset, data_folder):\n",
    "    global PROBLEM_SAMPLES\n",
    "    missing = []\n",
    "    matched = 0\n",
    "    matched_sources = []\n",
    "    matched_table_records = []\n",
    "    for row in known_subset.itertuples(index=False):\n",
    "        is_uniq = list(filter(lambda x: row.cell in x.name, sample_files))\n",
    "        if len(is_uniq) == 0:\n",
    "            missing.append(row.cell)\n",
    "        elif len(is_uniq) == 1:\n",
    "            matched += 1\n",
    "            matched_sources.append(row.source)\n",
    "            matched_table_records.append((row.cell, is_uniq[0]))\n",
    "        else:\n",
    "            pprint_mmatch = \"\\n\".join([sf.name for sf in sample_files])\n",
    "            raise ValueError(f\"Multi-match: {is_uniq} - {cell}\", pprint_mmatch)\n",
    "\n",
    "    if matched == 0:\n",
    "        raise ValueError(\"No files matched \", sample_files, known_subset)\n",
    "        \n",
    "    assert len(set(matched_sources)) == 1, matched_sources\n",
    "    source = matched_sources[0]\n",
    "        \n",
    "    if matched < len(sample_files):\n",
    "        sample_name = known_subset[\"sample\"].unique()\n",
    "        assert len(sample_name) == 1\n",
    "        sample_name = sample_name[0]\n",
    "        matched_files = set()\n",
    "        for cell, file_path in sorted(matched_table_records):\n",
    "            rel_path = file_path.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    cell,\n",
    "                    rel_path,\n",
    "                    \"match_found\",\n",
    "                    \"no-error\"\n",
    "                )\n",
    "            )\n",
    "            matched_files.add(file_path)\n",
    "        for sf in sample_files:\n",
    "            if sf in matched_files:\n",
    "                continue\n",
    "            rel_path = sf.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    \"no-cell\",\n",
    "                    rel_path,\n",
    "                    \"no-match-found\",\n",
    "                    \"unknown-file-on-share\"\n",
    "                )\n",
    "            )\n",
    "        print(\"ERROR\")\n",
    "        print(\"Unidentified sample files \", sample_name)\n",
    "        print(\"Folder \", data_folder)\n",
    "        return None, None, None\n",
    "    if missing:\n",
    "        sample_name = known_subset[\"sample\"].unique()\n",
    "        assert len(sample_name) == 1\n",
    "        sample_name = sample_name[0]\n",
    "        matched_cells = set()\n",
    "        for cell, file_path in sorted(matched_table_records):\n",
    "            rel_path = file_path.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    cell,\n",
    "                    str(rel_path),\n",
    "                    \"match-found\",\n",
    "                    \"no-error\"\n",
    "                )\n",
    "            )\n",
    "            matched_cells.add(cell)\n",
    "        for row in known_subset.loc[known_subset[\"source\"] == source, :].itertuples(index=False):\n",
    "            if row.cell in matched_cells:\n",
    "                continue\n",
    "            assert row.sample == sample_name\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    row.cell,\n",
    "                    row.sample,\n",
    "                    \"no-match-found\",\n",
    "                    \"missing-file-on-share\"\n",
    "                )\n",
    "            )\n",
    "        print(\"ERROR\")\n",
    "        print(\"Missing files on share \", sample_name)\n",
    "        print(\"Sample files: \", len(sample_files))\n",
    "        print(\"Known subset: \", known_subset.shape[0])\n",
    "        print(\"Known subset: \", known_subset)\n",
    "        print(\"Folder \", data_folder)\n",
    "        return None, None, None\n",
    "\n",
    "    sample_names = known_subset[\"sample\"].unique()\n",
    "    assert sample_names.size == 1\n",
    "    sample_name = sample_names[0]\n",
    "    if sample_name.startswith(\"GM\"):\n",
    "        sample_name = sample_name.replace(\"GM\", \"NA\")\n",
    "    \n",
    "    return sample_name, matched, source\n",
    "\n",
    "\n",
    "def write_verified_file(check_file, ts, subset, sample_name, fastq_paths, fofn_path):\n",
    "    \n",
    "    relpaths_fastq = sorted(\n",
    "        [f.relative_to(MOUNT.joinpath(TOP_LEVEL)) for f in fastq_paths]\n",
    "    )\n",
    "    relpaths_fastq = list(map(str, relpaths_fastq))\n",
    "    direct = all(sample_name in fq for fq in relpaths_fastq)\n",
    "    indirect = all(sample_name.replace(\"NA\", \"GM\") in fq for fq in relpaths_fastq)\n",
    "    assert direct or indirect, check_file\n",
    "    \n",
    "    with open(check_file, \"w\") as dump:\n",
    "        dump.write(f\"# {ts}\\n\")\n",
    "        subset.to_csv(dump, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "    fofn_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(fofn_path, \"w\") as fofn:\n",
    "        fofn.write(\"\\n\".join(relpaths_fastq) + \"\\n\")\n",
    "    return\n",
    "\n",
    "        \n",
    "def build_fofn_path(sample_name, data_type, project, source, ds_year):\n",
    "    \n",
    "    fofn_path = MOUNT.joinpath(\n",
    "        TOP_LEVEL, \"sample-centric\",\n",
    "        sample_name,\n",
    "        f\"{sample_name}_{data_type}_fastq.{project}-{source}-{ds_year}.fofn\"\n",
    "    )\n",
    "    return fofn_path\n",
    "\n",
    "DATA_TYPES = {\n",
    "        \"nanopore\": \"ont\",\n",
    "        \"pacbio_hifi\": \"hifi\"\n",
    "    }\n",
    "\n",
    "year = re.compile(\"20[0-9]{2}\")\n",
    "possible_years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "\n",
    "for project, data_folder in itt.product(project_folders, data_folders):\n",
    "    sample_folder_listings = MOUNT.joinpath(\n",
    "        TOP_LEVEL,\n",
    "        \"project-centric\",\n",
    "        project,\n",
    "        data_folder\n",
    "    )\n",
    "    cell_lut = hifi_cells if data_folder == \"pacbio_hifi\" else ont_cells\n",
    "    data_type = DATA_TYPES[data_folder]\n",
    "    if not sample_folder_listings.is_dir():\n",
    "        print(\"skipping, no dir \", sample_folder_listings)\n",
    "    for sample_folder_lst in sample_folder_listings.glob(\"**/sample-folder.lst\"):\n",
    "        if any(cf in str(sample_folder_lst) for cf in curated_folder_names):\n",
    "            folder_is_curated = True\n",
    "        else:\n",
    "            folder_is_curated = False\n",
    "        with open(sample_folder_lst, \"r\") as listing:\n",
    "            for line in listing:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                sample_folder = MOUNT.joinpath(TOP_LEVEL, line.strip())\n",
    "                mobj = year.search(line)\n",
    "                if mobj is None:\n",
    "                    print(\"no year \", line.strip())\n",
    "                    ds_year = \"20XX\"\n",
    "                else:\n",
    "                    ds_year = mobj.group(0)\n",
    "                    assert ds_year in possible_years\n",
    "                \n",
    "                unsorted_fastq = load_fastq_files(sample_folder)\n",
    "                # group sample files by data source to prevent\n",
    "                # missing file exceptions\n",
    "                sample_file_groups = group_files_by_sample_and_source(\n",
    "                    unsorted_fastq,\n",
    "                    cell_lut,\n",
    "                    folder_is_curated\n",
    "                )\n",
    "                for (sample, source), fastq_files in sample_file_groups.items():\n",
    "                    sample_num = \"SIN:\" + sample[2:]\n",
    "                    select_sample = cell_lut[\"sin\"] == sample_num\n",
    "                    select_source = cell_lut[\"source\"] == source\n",
    "                    subset = cell_lut.loc[select_sample & select_source, :]\n",
    "                    sample_name, matched_files, matched_source = find_matching(\n",
    "                        fastq_files, subset, sample_folder_listings\n",
    "                    )\n",
    "                    if sample_name is None:\n",
    "                        continue\n",
    "                    # not raising = dataset complete\n",
    "                    check_file = sample_folder.joinpath(\n",
    "                        f\"{sample_name}.{matched_files}-cells.verified\"\n",
    "                    )\n",
    "                    fofn_path = build_fofn_path(\n",
    "                        sample_name, data_type,\n",
    "                        project, matched_source, ds_year\n",
    "                    )\n",
    "                    if DRYRUN:\n",
    "                        if VERBOSE:\n",
    "                            print(\"Would create VERIFY: \", check_file)\n",
    "                            print(\"Would create FOFN: \", fofn_path)\n",
    "                    else:\n",
    "                        if VERBOSE:\n",
    "                            print(\"Creating: \", check_file)\n",
    "                            print(\"Creating: \", fofn_path)\n",
    "                        write_verified_file(\n",
    "                            check_file,\n",
    "                            TIMESTAMP,\n",
    "                            subset,\n",
    "                            sample_name,\n",
    "                            fastq_files,\n",
    "                            fofn_path\n",
    "                        )\n",
    "                        cell_lut.loc[subset.index, \"HHU_complete\"] = \"yes\"\n",
    "\n",
    "\n",
    "prs = pd.DataFrame.from_records(\n",
    "    PROBLEM_SAMPLES,\n",
    "    columns=[\"sample_name\", \"cell_id\", \"path_or_annotated_sample\", \"file_cell_matched\", \"error_source\"]\n",
    ")\n",
    "prs.sort_values([\"sample_name\", \"cell_id\"], inplace=True)\n",
    "prs.to_csv(error_out, header=True, index=False, sep=\"\\t\")                     \n",
    "\n",
    "if not DRYRUN:\n",
    "    merged = pd.concat([hifi_cells, ont_cells], axis=0, ignore_index=False)\n",
    "    merged = merged.merge(sample_order, on=\"sin\", how=\"inner\")\n",
    "    merged.sort_values([\"sample_num\", \"sin\", \"read_type\", \"source\", \"cell\"], inplace=True)\n",
    "    reorder_header = [\n",
    "        \"sample_num\", \"sample\", \"read_type\", \"source\",\n",
    "        \"cell\", \"project_phase\", \"data_production\", \"HHU_complete\", \"sin\"\n",
    "    ]\n",
    "    merged.loc[merged[\"data_production\"].isin([\"error\", \"incomplete\"]), \"HHU_complete\"] = \"no\"\n",
    "    merged = merged[reorder_header]\n",
    "    with open(clean_out, \"w\") as dump:\n",
    "        _ = dump.write(f\"# {TIMESTAMP}\\n\")\n",
    "        merged.to_csv(dump, header=True, index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
