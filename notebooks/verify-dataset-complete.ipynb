{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd583875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Unidentified sample files  ['GM19129'] 7 6\n",
      "ERROR\n",
      "Unidentified sample files  ['GM20355'] 25 20\n",
      "ERROR\n",
      "Unidentified sample files  ['GM21487'] 14 12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 247\u001b[0m\n\u001b[1;32m    245\u001b[0m fastq, fastq_names \u001b[38;5;241m=\u001b[39m load_fastq_files(sample_folder)\n\u001b[1;32m    246\u001b[0m subset \u001b[38;5;241m=\u001b[39m cell_lut\u001b[38;5;241m.\u001b[39mloc[cell_lut[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m sample_num, :]\n\u001b[0;32m--> 247\u001b[0m sample_name, matched_files, matched_source \u001b[38;5;241m=\u001b[39m \u001b[43mfind_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfastq_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [16], line 110\u001b[0m, in \u001b[0;36mfind_matching\u001b[0;34m(sample_files, known_subset)\u001b[0m\n\u001b[1;32m    108\u001b[0m is_uniq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(is_uniq)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_uniq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 110\u001b[0m     missing\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcell\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_uniq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    112\u001b[0m     matched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cell' is not defined"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "import itertools as itt\n",
    "\n",
    "ts = dt.datetime.now()\n",
    "TIMESTAMP = ts.strftime(\"%Y%m%dT%H%M\")\n",
    "\n",
    "MOUNT = pl.Path(\"/mounts/hilbert/project\")\n",
    "TOP_LEVEL = pl.Path(\"projects/medbioinf/data/00_RESTRUCTURE\")\n",
    "project_folders = [\"hgsvc\", \"unknown\"]\n",
    "data_folders = [\"nanopore\", \"pacbio_hifi\"]\n",
    "\n",
    "cell_metadata = pl.Path(\"/home/ebertp/work/code/cubi/project-run-hgsvc-hybrid-assemblies/annotations/external\")\n",
    "\n",
    "hifi_cells = cell_metadata.glob(\"*hifi*.tsv\")\n",
    "ont_cells = cell_metadata.glob(\"*ont*.tsv\")\n",
    "\n",
    "clean_out = cell_metadata.parent.joinpath(\"hgsvc_cells.tsv\")\n",
    "\n",
    "\n",
    "def load_cell_metadata(fpath):\n",
    "    \n",
    "    source_must_exist = False\n",
    "    if \"jax\" in fpath.name:\n",
    "        source = \"JAX\"\n",
    "    elif \"uwash\" in fpath.name or \"UW\" in fpath.name:\n",
    "        source = \"UW\"\n",
    "    elif \"umigs\" in fpath.name:\n",
    "        source = \"UMIGS\"\n",
    "    else:\n",
    "        source_must_exist = True\n",
    "        #raise ValueError(f\"Unknown project {fpath.name}\")\n",
    "    if \"hifi\" in fpath.name:\n",
    "        read_type = \"HiFi\"\n",
    "    elif \"ont\" in fpath.name:\n",
    "        read_type = \"ONT\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown read type: {fpath.name}\")\n",
    "    \n",
    "    df = pd.read_csv(fpath, header=0, sep=\"\\t\")\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if source_must_exist:\n",
    "        assert \"source\" in df.columns\n",
    "        df = df[[\"sample\", \"cell\", \"source\"]]\n",
    "    else:\n",
    "        df = df[[\"sample\", \"cell\"]]\n",
    "        df[\"source\"] = source\n",
    "    df[\"sample\"] = df[\"sample\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.extract(\"([A-Z\\-_a-z0-9]+)\")\n",
    "    df[\"sin\"] = \"SIN:\" + df[\"sample\"].str.extract(\"([0-9]+)\") \n",
    "    df[\"read_type\"] = read_type\n",
    "    return df\n",
    "    \n",
    "hifi_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in hifi_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "hifi_exp_count = hifi_cells[\"sin\"].value_counts().to_dict()\n",
    "hifi_cells[\"exp_count\"] = hifi_cells[\"sin\"].replace(hifi_exp_count)\n",
    "\n",
    "ont_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in ont_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "ont_exp_count = ont_cells[\"sin\"].value_counts().to_dict()\n",
    "ont_cells[\"exp_count\"] = ont_cells[\"sin\"].replace(ont_exp_count)\n",
    "\n",
    "hifi_cells[\"HHU_complete\"] = \"no\"\n",
    "ont_cells[\"HHU_complete\"] = \"no\"\n",
    "hifi_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "hifi_cells.reset_index(drop=True, inplace=True)\n",
    "ont_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "ont_cells.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def group_files_by_sample(fastq_files, all_known):\n",
    "    \n",
    "    sample_file_groups = col.defaultdict(list)\n",
    "    for fq in fastq_files:\n",
    "        matches = []\n",
    "        for row in all_known.itertuples():\n",
    "            if row.cell not in fq.name:\n",
    "                continue\n",
    "            matches.append((row.sample, row.cell, fq))\n",
    "        if len(matches) > 1:\n",
    "            for smp, cell, fp in matches:\n",
    "                print(smp, ' - ', cell, ' - ', fp.name)\n",
    "            raise ValueError(\"Multi-match\")\n",
    "        elif len(matches) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sample, cell_id, file_path = matches[0]\n",
    "            sample_file_groups[sample].append(file_path)\n",
    "    return sample_file_groups\n",
    "\n",
    "\n",
    "def find_matching(sample_files, known_subset):\n",
    "    \n",
    "    missing = []\n",
    "    matched = 0\n",
    "    matched_sources = []\n",
    "    for row in known_subset.itertuples(index=False):\n",
    "        is_uniq = [row.cell in fn for fn in sample_files]\n",
    "        is_uniq = sum(is_uniq)\n",
    "        if is_uniq == 0:\n",
    "            missing.append(cell)\n",
    "        elif is_uniq == 1:\n",
    "            matched += 1\n",
    "            matched_sources.append(row.source)\n",
    "        else:\n",
    "            pprint_mmatch = \"\\n\".join([sf.name for sf in sample_files])\n",
    "            raise ValueError(f\"Multi-match: {is_uniq} - {cell}\", pprint_mmatch)\n",
    "\n",
    "    if matched == 0:\n",
    "        raise ValueError(\"No files matched \", sample_files, know_subset)\n",
    "    if matched != len(sample_files):\n",
    "        sample_name = known_subset[\"sample\"].unique()\n",
    "        print(\"ERROR\")\n",
    "        print(\"Unidentified sample files \", sample_name, len(sample_files), known_subset.shape[0])\n",
    "        return None, None, None\n",
    "    if missing:\n",
    "        raise ValueError(\"Missing sample files \", sample_files, known_subset)\n",
    "    sample_names = known_subset[\"sample\"].unique()\n",
    "    assert sample_names.size == 1\n",
    "    sample_name = sample_names[0]\n",
    "    if sample_name.startswith(\"GM\"):\n",
    "        sample_name = sample_name.replace(\"GM\", \"NA\")\n",
    "    assert len(set(matched_sources)) == 1, matched_sources\n",
    "    return sample_name, matched, matched_sources[0]\n",
    "\n",
    "\n",
    "def load_fastq_files(folder_path):\n",
    "    \n",
    "    all_files = list(folder_path.glob(\"**/*.fastq.gz\"))\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"no fastqs at {folder_path}\")\n",
    "    pass_files = [fp for fp in all_files if \"fail\" not in fp.name]\n",
    "    all_names = [fp.name for fp in pass_files]\n",
    "    return pass_files, all_names\n",
    "\n",
    "\n",
    "\n",
    "def write_verified_file(check_file, ts, subset, fastq_paths, fofn_path):\n",
    "    \n",
    "    \n",
    "    with open(check_file, \"w\") as dump:\n",
    "        dump.write(f\"# {ts}\\n\")\n",
    "        subset.to_csv(dump, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "    relpaths_fastq = sorted(\n",
    "        [f.relative_to(MOUNT.joinpath(TOP_LEVEL)) for f in fastq_paths]\n",
    "    )\n",
    "    relpaths_fastq = list(map(str, relpaths_fastq))\n",
    "\n",
    "    fofn_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(fofn_path, \"w\") as fofn:\n",
    "        fofn.write(\"\\n\".join(relpaths_fastq) + \"\\n\")\n",
    "    return\n",
    "\n",
    "        \n",
    "def build_fofn_path(sample_name, data_type, project, source, ds_year):\n",
    "    \n",
    "    fofn_path = MOUNT.joinpath(\n",
    "        TOP_LEVEL, \"sample-centric\",\n",
    "        sample_name,\n",
    "        f\"{sample_name}_{data_type}_fastq.{project}-{source}-{ds_year}.fofn\"\n",
    "    )\n",
    "    return fofn_path\n",
    "\n",
    "data_types = {\n",
    "        \"nanopore\": \"ont\",\n",
    "        \"pacbio_hifi\": \"hifi\"\n",
    "    }\n",
    "\n",
    "year = re.compile(\"20[0-9]{2}\")\n",
    "possible_years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "\n",
    "for project, data_folder in itt.product(project_folders, data_folders):\n",
    "    sample_folder_listings = mount.joinpath(\n",
    "        TOP_LEVEL,\n",
    "        \"project-centric\",\n",
    "        project,\n",
    "        data_folder\n",
    "    )\n",
    "    cell_lut = hifi_cells if data_folder == \"pacbio_hifi\" else ont_cells\n",
    "    data_type = data_types[data_folder]\n",
    "    assert sample_folder_listings.is_dir()\n",
    "    for sample_folder_lst in sample_folder_listings.glob(\"**/sample-folder.lst\"):\n",
    "        with open(sample_folder_lst, \"r\") as listing:\n",
    "            for line in listing:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                sample_folder = mount.joinpath(TOP_LEVEL, line.strip())\n",
    "                mobj = year.search(line)\n",
    "                if mobj is None:\n",
    "                    print(\"no year \", line.strip())\n",
    "                    ds_year = \"20XX\"\n",
    "                else:\n",
    "                    ds_year = mobj.group(0)\n",
    "                    assert ds_year in possible_years\n",
    "                sample_num = \"SIN:\" + sample_folder.name[2:]\n",
    "                if sample_num not in cell_lut[\"sin\"].values:\n",
    "                    continue\n",
    "                    unsorted_fastq, _ = load_fastq_files(sample_folder)\n",
    "                    \n",
    "                    sample_file_groups = group_files_by_sample(\n",
    "                        unsorted_fastq,\n",
    "                        cell_lut\n",
    "                    )\n",
    "                    for sample, sample_files in sample_file_groups.items():\n",
    "                        sample_num = \"SIN:\" + sample[2:]\n",
    "                        fastq_names = [sf.name for sf in sample_files]\n",
    "                        subset = cell_lut.loc[cell_lut[\"sin\"] == sample_num, :]\n",
    "                        sample_name, matched_files, matched_source = find_matching(fastq_names, subset)\n",
    "                        if sample_name is None:\n",
    "                            continue\n",
    "                        # not raising = dataset complete\n",
    "                        check_file = sample_folder.joinpath(\n",
    "                            f\"{sample_name}.{matched_files}-cells.verified\"\n",
    "                        )\n",
    "                        if False:  # not check_file.is_file():\n",
    "                            with open(check_file, \"w\") as dump:\n",
    "                                dump.write(f\"# {ts}\\n\")\n",
    "                                subset.to_csv(dump, sep=\"\\t\", header=True, index=False)\n",
    "                            relpaths_fastq = sorted(\n",
    "                                [f.relative_to(mount.joinpath(subfolder)) for f in sample_files]\n",
    "                            )\n",
    "                            relpaths_fastq = list(map(str, relpaths_fastq))\n",
    "                            fofn_path = mount.joinpath(\n",
    "                                subfolder, \"sample-centric\", sample_name,\n",
    "                                f\"{sample_name}_{data_types[data_folder]}_fastq.hgsvc-{ds_year}.fofn\"\n",
    "                            )\n",
    "                            fofn_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "                            with open(fofn_path, \"w\") as fofn:\n",
    "                                fofn.write(\"\\n\".join(relpaths_fastq) + \"\\n\")\n",
    "                        cell_lut.loc[subset.index, \"HHU_complete\"] = \"yes\"\n",
    "                        \n",
    "                else:\n",
    "                    # easy case: data per sample in subfolder\n",
    "                    fastq, fastq_names = load_fastq_files(sample_folder)\n",
    "                    subset = cell_lut.loc[cell_lut[\"sin\"] == sample_num, :]\n",
    "                    sample_name, matched_files, matched_source = find_matching(fastq_names, subset)\n",
    "                    if sample_name is None:\n",
    "                        continue\n",
    "                    # not raising = dataset complete\n",
    "                    check_file = sample_folder.joinpath(\n",
    "                        f\"{sample_name}.{matched_files}-cells.verified\"\n",
    "                    )\n",
    "                    if True:  # not check_file.is_file():\n",
    "                        fofn_path = build_fofn_path(\n",
    "                            sample_name, data_type,\n",
    "                            project, matched_source, ds_year\n",
    "                        )\n",
    "                        write_verified_file(\n",
    "                            check_file,\n",
    "                            TIMESTAMP,\n",
    "                            subset,\n",
    "                            fastq,\n",
    "                            fofn_path\n",
    "                        )\n",
    "                    cell_lut.loc[subset.index, \"HHU_complete\"] = \"yes\"\n",
    "                \n",
    "merged = pd.concat([hifi_cells, ont_cells], axis=0, ignore_index=False)\n",
    "merged.sort_values([\"sin\", \"read_type\", \"source\", \"cell\"], inplace=True)\n",
    "merged.to_csv(clean_out, header=True, index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
