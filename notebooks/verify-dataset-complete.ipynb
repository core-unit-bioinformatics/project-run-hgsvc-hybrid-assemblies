{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd583875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Missing files on share  HG02769\n",
      "Sample files:  22\n",
      "Known subset:  23\n",
      "ERROR\n",
      "Missing files on share  HG00732\n",
      "Sample files:  3\n",
      "Known subset:  9\n",
      "no year  project-centric/unknown/nanopore/HG00733\n",
      "no year  project-centric/unknown/nanopore/NA24385_HG002\n",
      "no year  project-centric/unknown/pacbio_hifi/HG00733\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA586863/nanopore\n",
      "no year  project-centric/PRJNA586863/pacbio_hifi\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA731524/nanopore\n",
      "no year  project-centric/PRJNA731524/pacbio_hifi\n",
      "skipping, no dir  /mounts/hilbert/project/projects/medbioinf/data/00_RESTRUCTURE/project-centric/PRJNA813010/nanopore\n",
      "no year  project-centric/PRJNA813010/pacbio_hifi\n"
     ]
    }
   ],
   "source": [
    "import collections as col\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "\n",
    "import itertools as itt\n",
    "\n",
    "DRYRUN = False\n",
    "VERBOSE = False\n",
    "\n",
    "ts = dt.datetime.now()\n",
    "TIMESTAMP = ts.strftime(\"%Y%m%dT%H%M\")\n",
    "\n",
    "MOUNT = pl.Path(\"/mounts/hilbert/project\")\n",
    "TOP_LEVEL = pl.Path(\"projects/medbioinf/data/00_RESTRUCTURE\")\n",
    "project_folders = [\n",
    "    \"hgsvc\", \"unknown\", \"PRJNA586863\", \"PRJNA731524\", \"PRJNA813010\"\n",
    "]\n",
    "data_folders = [\"nanopore\", \"pacbio_hifi\"]\n",
    "\n",
    "cell_metadata = pl.Path(\"/home/ebertp/work/code/cubi/project-run-hgsvc-hybrid-assemblies/annotations/external\")\n",
    "\n",
    "hifi_cells = cell_metadata.glob(\"*hifi*.tsv\")\n",
    "ont_cells = cell_metadata.glob(\"*ont*.tsv\")\n",
    "\n",
    "relabel_file = cell_metadata.joinpath(\"relabel_sources.tsv\")\n",
    "\n",
    "clean_out = cell_metadata.parent.joinpath(\"hgsvc_cells.tsv\")\n",
    "error_out = cell_metadata.parent.joinpath(\"errors.tsv\")\n",
    "\n",
    "PROBLEM_SAMPLES = []\n",
    "\n",
    "def load_cell_metadata(fpath):\n",
    "    \n",
    "    source_must_exist = False\n",
    "    if \"jax\" in fpath.name:\n",
    "        source = \"JAX\"\n",
    "    elif \"uwash\" in fpath.name or \"UW\" in fpath.name:\n",
    "        source = \"UW\"\n",
    "    elif \"umigs\" in fpath.name:\n",
    "        source = \"UMIGS\"\n",
    "    else:\n",
    "        source_must_exist = True\n",
    "        #raise ValueError(f\"Unknown project {fpath.name}\")\n",
    "    if \"hifi\" in fpath.name:\n",
    "        read_type = \"HiFi\"\n",
    "    elif \"ont\" in fpath.name:\n",
    "        read_type = \"ONT\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown read type: {fpath.name}\")\n",
    "    \n",
    "    df = pd.read_csv(fpath, header=0, sep=\"\\t\")\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if source_must_exist:\n",
    "        assert \"source\" in df.columns\n",
    "        df = df[[\"sample\", \"cell\", \"source\"]]\n",
    "    else:\n",
    "        df = df[[\"sample\", \"cell\"]]\n",
    "        df[\"source\"] = source\n",
    "    df[\"sample\"] = df[\"sample\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.strip()\n",
    "    df[\"cell\"] = df[\"cell\"].str.extract(\"([A-Z\\-_a-z0-9]+)\")\n",
    "    df[\"sin\"] = \"SIN:\" + df[\"sample\"].str.extract(\"([0-9]+)\") \n",
    "    df[\"read_type\"] = read_type\n",
    "    df[\"annotation_source\"] = fpath.name\n",
    "    return df\n",
    "\n",
    "\n",
    "def relabel_sources(labeling, cell_table):\n",
    "    \n",
    "    new_labels = pd.read_csv(labeling, header=0, sep=\"\\t\")\n",
    "    new_labels[\"cell\"] = new_labels[\"cell\"].str.strip()\n",
    "    \n",
    "    for row in new_labels.itertuples(index=False):\n",
    "        select_sample = cell_table[\"sample\"] == row.sample\n",
    "        select_cell = cell_table[\"cell\"].str.contains(row.cell)\n",
    "        select_reads = cell_table[\"read_type\"] == row.read_type\n",
    "        selector = select_sample & select_cell & select_reads\n",
    "        cell_table.loc[selector, \"source\"] = row.source\n",
    "    return cell_table\n",
    "\n",
    "\n",
    "hifi_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in hifi_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "hifi_cells = relabel_sources(relabel_file, hifi_cells)\n",
    "\n",
    "ont_cells = pd.concat(\n",
    "    [load_cell_metadata(fp) for fp in ont_cells],\n",
    "    axis=0, ignore_index=False\n",
    ")\n",
    "ont_cells = relabel_sources(relabel_file, ont_cells)\n",
    "\n",
    "hifi_cells[\"HHU_complete\"] = \"no\"\n",
    "ont_cells[\"HHU_complete\"] = \"no\"\n",
    "hifi_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "hifi_cells.reset_index(drop=True, inplace=True)\n",
    "ont_cells.sort_values([\"sin\", \"cell\"], inplace=True)\n",
    "ont_cells.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def group_files_by_sample_and_source(fastq_files, all_known):\n",
    "    \n",
    "    sample_file_groups = col.defaultdict(list)\n",
    "    for fq in fastq_files:\n",
    "        matches = []\n",
    "        for row in all_known.itertuples():\n",
    "            if row.cell not in fq.name:\n",
    "                continue\n",
    "            matches.append((row.sample, row.cell, row.source, fq, row.annotation_source))\n",
    "        if len(matches) > 1:\n",
    "            for smp, cell, source, fp, annfile in matches:\n",
    "                print(smp, ' - ', cell, ' - ', source, ' - ', fp.name, ' - ', annfile)\n",
    "            raise ValueError(\"Multi-match\")\n",
    "        elif len(matches) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sample, cell_id, source, file_path, annfile = matches[0]\n",
    "            sample_file_groups[(sample, source)].append(file_path)\n",
    "    return sample_file_groups\n",
    "\n",
    "\n",
    "def find_matching(sample_files, known_subset, data_folder):\n",
    "    global PROBLEM_SAMPLES\n",
    "    missing = []\n",
    "    matched = 0\n",
    "    matched_sources = []\n",
    "    matched_table_records = []\n",
    "    for row in known_subset.itertuples(index=False):\n",
    "        is_uniq = list(filter(lambda x: row.cell in x.name, sample_files))\n",
    "        if len(is_uniq) == 0:\n",
    "            missing.append(row.cell)\n",
    "        elif len(is_uniq) == 1:\n",
    "            matched += 1\n",
    "            matched_sources.append(row.source)\n",
    "            matched_table_records.append((row.cell, is_uniq[0]))\n",
    "        else:\n",
    "            pprint_mmatch = \"\\n\".join([sf.name for sf in sample_files])\n",
    "            raise ValueError(f\"Multi-match: {is_uniq} - {cell}\", pprint_mmatch)\n",
    "\n",
    "    if matched == 0:\n",
    "        raise ValueError(\"No files matched \", sample_files, known_subset)\n",
    "        \n",
    "    assert len(set(matched_sources)) == 1, matched_sources\n",
    "    source = matched_sources[0]\n",
    "        \n",
    "    if matched < len(sample_files):\n",
    "        sample_name = known_subset[\"sample\"].unique()\n",
    "        assert len(sample_name) == 1\n",
    "        sample_name = sample_name[0]\n",
    "        matched_files = set()\n",
    "        for cell, file_path in sorted(matched_table_records):\n",
    "            rel_path = file_path.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    cell,\n",
    "                    rel_path,\n",
    "                    \"match_found\",\n",
    "                    \"no-error\"\n",
    "                )\n",
    "            )\n",
    "            matched_files.add(file_path)\n",
    "        for sf in sample_files:\n",
    "            if sf in matched_files:\n",
    "                continue\n",
    "            rel_path = sf.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    \"no-cell\",\n",
    "                    rel_path,\n",
    "                    \"no-match-found\",\n",
    "                    \"unknown-file-on-share\"\n",
    "                )\n",
    "            )\n",
    "        print(\"ERROR\")\n",
    "        print(\"Unidentified sample files \", sample_name)\n",
    "        return None, None, None\n",
    "    if missing:\n",
    "        sample_name = known_subset[\"sample\"].unique()\n",
    "        assert len(sample_name) == 1\n",
    "        sample_name = sample_name[0]\n",
    "        matched_cells = set()\n",
    "        for cell, file_path in sorted(matched_table_records):\n",
    "            rel_path = file_path.relative_to(data_folder)\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    cell,\n",
    "                    str(rel_path),\n",
    "                    \"match-found\",\n",
    "                    \"no-error\"\n",
    "                )\n",
    "            )\n",
    "            matched_cells.add(cell)\n",
    "        for row in known_subset.loc[known_subset[\"source\"] == source, :].itertuples(index=False):\n",
    "            if row.cell in matched_cells:\n",
    "                continue\n",
    "            assert row.sample == sample_name\n",
    "            PROBLEM_SAMPLES.append(\n",
    "                (\n",
    "                    sample_name,\n",
    "                    row.cell,\n",
    "                    row.sample,\n",
    "                    \"no-match-found\",\n",
    "                    \"missing-file-on-share\"\n",
    "                )\n",
    "            )\n",
    "        print(\"ERROR\")\n",
    "        print(\"Missing files on share \", sample_name)\n",
    "        print(\"Sample files: \", len(sample_files))\n",
    "        print(\"Known subset: \", known_subset.shape[0])\n",
    "        return None, None, None\n",
    "\n",
    "    sample_names = known_subset[\"sample\"].unique()\n",
    "    assert sample_names.size == 1\n",
    "    sample_name = sample_names[0]\n",
    "    if sample_name.startswith(\"GM\"):\n",
    "        sample_name = sample_name.replace(\"GM\", \"NA\")\n",
    "    \n",
    "    return sample_name, matched, source\n",
    "\n",
    "\n",
    "def load_fastq_files(folder_path):\n",
    "    \n",
    "    all_files = list(folder_path.glob(\"**/*.fastq.gz\"))\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"no fastqs at {folder_path}\")\n",
    "    pass_files = [fp for fp in all_files if \"fail\" not in fp.name]\n",
    "    all_names = [fp.name for fp in pass_files]\n",
    "    return pass_files\n",
    "\n",
    "\n",
    "def write_verified_file(check_file, ts, subset, sample_name, fastq_paths, fofn_path):\n",
    "    \n",
    "    relpaths_fastq = sorted(\n",
    "        [f.relative_to(MOUNT.joinpath(TOP_LEVEL)) for f in fastq_paths]\n",
    "    )\n",
    "    relpaths_fastq = list(map(str, relpaths_fastq))\n",
    "    direct = all(sample_name in fq for fq in relpaths_fastq)\n",
    "    indirect = all(sample_name.replace(\"NA\", \"GM\") in fq for fq in relpaths_fastq)\n",
    "    assert direct or indirect, check_file\n",
    "    \n",
    "    with open(check_file, \"w\") as dump:\n",
    "        dump.write(f\"# {ts}\\n\")\n",
    "        subset.to_csv(dump, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "    fofn_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(fofn_path, \"w\") as fofn:\n",
    "        fofn.write(\"\\n\".join(relpaths_fastq) + \"\\n\")\n",
    "    return\n",
    "\n",
    "        \n",
    "def build_fofn_path(sample_name, data_type, project, source, ds_year):\n",
    "    \n",
    "    fofn_path = MOUNT.joinpath(\n",
    "        TOP_LEVEL, \"sample-centric\",\n",
    "        sample_name,\n",
    "        f\"{sample_name}_{data_type}_fastq.{project}-{source}-{ds_year}.fofn\"\n",
    "    )\n",
    "    return fofn_path\n",
    "\n",
    "DATA_TYPES = {\n",
    "        \"nanopore\": \"ont\",\n",
    "        \"pacbio_hifi\": \"hifi\"\n",
    "    }\n",
    "\n",
    "year = re.compile(\"20[0-9]{2}\")\n",
    "possible_years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "\n",
    "for project, data_folder in itt.product(project_folders, data_folders):\n",
    "    sample_folder_listings = MOUNT.joinpath(\n",
    "        TOP_LEVEL,\n",
    "        \"project-centric\",\n",
    "        project,\n",
    "        data_folder\n",
    "    )\n",
    "    cell_lut = hifi_cells if data_folder == \"pacbio_hifi\" else ont_cells\n",
    "    data_type = DATA_TYPES[data_folder]\n",
    "    if not sample_folder_listings.is_dir():\n",
    "        print(\"skipping, no dir \", sample_folder_listings)\n",
    "    for sample_folder_lst in sample_folder_listings.glob(\"**/sample-folder.lst\"):\n",
    "        with open(sample_folder_lst, \"r\") as listing:\n",
    "            for line in listing:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                sample_folder = MOUNT.joinpath(TOP_LEVEL, line.strip())\n",
    "                mobj = year.search(line)\n",
    "                if mobj is None:\n",
    "                    print(\"no year \", line.strip())\n",
    "                    ds_year = \"20XX\"\n",
    "                else:\n",
    "                    ds_year = mobj.group(0)\n",
    "                    assert ds_year in possible_years\n",
    "                \n",
    "                unsorted_fastq = load_fastq_files(sample_folder)\n",
    "                # group sample files by data source to prevent\n",
    "                # missing file exceptions\n",
    "                sample_file_groups = group_files_by_sample_and_source(\n",
    "                    unsorted_fastq,\n",
    "                    cell_lut\n",
    "                )\n",
    "                for (sample, source), fastq_files in sample_file_groups.items():\n",
    "                    sample_num = \"SIN:\" + sample[2:]\n",
    "                    select_sample = cell_lut[\"sin\"] == sample_num\n",
    "                    select_source = cell_lut[\"source\"] == source\n",
    "                    subset = cell_lut.loc[select_sample & select_source, :]\n",
    "                    sample_name, matched_files, matched_source = find_matching(\n",
    "                        fastq_files, subset, sample_folder_listings\n",
    "                    )\n",
    "                    if sample_name is None:\n",
    "                        continue\n",
    "                    # not raising = dataset complete\n",
    "                    check_file = sample_folder.joinpath(\n",
    "                        f\"{sample_name}.{matched_files}-cells.verified\"\n",
    "                    )\n",
    "                    fofn_path = build_fofn_path(\n",
    "                        sample_name, data_type,\n",
    "                        project, matched_source, ds_year\n",
    "                    )\n",
    "                    if DRYRUN:\n",
    "                        if VERBOSE:\n",
    "                            print(\"Would create VERIFY: \", check_file)\n",
    "                            print(\"Would create FOFN: \", fofn_path)\n",
    "                    else:\n",
    "                        if VERBOSE:\n",
    "                            print(\"Creating: \", check_file)\n",
    "                            print(\"Creating: \", fofn_path)\n",
    "                        write_verified_file(\n",
    "                            check_file,\n",
    "                            TIMESTAMP,\n",
    "                            subset,\n",
    "                            sample_name,\n",
    "                            fastq_files,\n",
    "                            fofn_path\n",
    "                        )\n",
    "                        cell_lut.loc[subset.index, \"HHU_complete\"] = \"yes\"\n",
    "\n",
    "\n",
    "prs = pd.DataFrame.from_records(\n",
    "    PROBLEM_SAMPLES,\n",
    "    columns=[\"sample_name\", \"cell_id\", \"path_or_annotated_sample\", \"file_cell_matched\", \"error_source\"]\n",
    ")\n",
    "prs.sort_values([\"sample_name\", \"cell_id\"], inplace=True)\n",
    "prs.to_csv(error_out, header=True, index=False, sep=\"\\t\")                     \n",
    "\n",
    "if not DRYRUN:\n",
    "    merged = pd.concat([hifi_cells, ont_cells], axis=0, ignore_index=False)\n",
    "    merged.sort_values([\"sin\", \"read_type\", \"source\", \"cell\"], inplace=True)\n",
    "    merged.to_csv(clean_out, header=True, index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
